{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 TensorFlow (tf)\n",
        "\n",
        "- 파이썬 기반의 머신러닝 플랫폼\n",
        "- 수치 텐서에 대한 수학적 표현을 적용하는 것이 핵심\n",
        "  \n",
        "  - 미분 가능한 어떤 표현식에 대해서도 자동으로 gradient 계산\n",
        "  - GPU, TPU 적용 가능\n",
        "  - tensorflow에서 정의한 계산의 머신 분산 용이\n",
        "  - 다른 runtime 에 맞도록 변환할 수 있어 (TensorFlow Lite) 실전환경에 쉽게 배포 가능"
      ],
      "metadata": {
        "id": "wyIZT3o0z8O4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Keras\n",
        "\n",
        "- tensorflow 위에 구축된 파이썬용 딥러닝 API\n",
        "- 딥러닝 모델 구축과 훈련에 대한 방법 제공\n",
        "- 사용자 층이 다양하여 프레임워크 차원에서의 조작할 수 도 있으며, 세부내용에 대해 완전히 제어하는 것도 가능\n",
        "\n",
        "![keras with tensorflow](https://wikidocs.net/images/page/164735/Fig_05.png)\n",
        "\n",
        "그림 출처 : [wikidocs.net](https://wikidocs.net/164735)"
      ],
      "metadata": {
        "id": "9Ny8wk9e055L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 케라스와 텐서플로의 간략한 역사\n",
        "\n",
        "- keras는 Theano(th, tf와 같은 텐서조작 라이브러리)를 위한 라이브러리로 개발\n",
        "- 2015년 multibackend 구조로 refactoring 되어 theano와 tensorflow에서 모두 사용가능하게 됨\n",
        "- 2017년 CNTK, MXNet에 대해서도 backend 옵션이 추가 (현재 Theano, CNTK 개발 중단)\n",
        "- 2018년 tensorflow의 공식 고수준 API로 선정\n",
        "\n",
        "> 고수준 API : 다양한 parameter를 별도 선언할 필요없이 사용할 수 있도록 조성된 api [참고](https://m.blog.naver.com/worb1605/221407983135)\n",
        "\n",
        "- 2019년 keras가 주축이 된 tf2 가 relase 됨"
      ],
      "metadata": {
        "id": "py6URzO3z7hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### `From tensorflow import keras` vs `import keras`\n",
        "\n",
        "tf 2.0 에서는 다음 사항들을 핵심 특징으로 여기고 있으며 이들은 keras API를 적극적으로 운영하면서 가능해진 사항들임\n",
        "- Sessions and eager execution\n",
        "- Automatic differentiation\n",
        "- Model and layer subclassing\n",
        "- Better multi-GPU/distributed training support\n",
        "\n",
        "\n",
        "keras는 딥러닝 구축 및 훈련을 위한 method를 제안하지만 그 연산 자체는 할 수 없어 텐서 연산을 위한 backend (tf, th)를 필요로 함  \n",
        "\n",
        "tf 1.10.0 에서 keras는 직접 통합되는 방식으로 소속 되었으나 별도의 확장성을 위하여 keras library와의 독립성을 유지.  \n",
        "tf 2.0 에서부터는 keras는 완전 종속된 상위 API로 정의 되었으며\n",
        "자체 라이브러리로는 Keras 2.3.0 을 마지막으로 relase 함.\n",
        "\n",
        "이전 import keras로 사용하였던 코드들을 tf.keras로 변경하는 것을 권장하며 앞으로의 코드에서도 tf.keras를 사용할 것을 공지함.\n",
        "\n",
        "출처 : [https://pyimagesearch.com/](https://pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/)"
      ],
      "metadata": {
        "id": "kH0JwUlL5faF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 딥러닝 작업 환경 설정하기\n",
        "\n",
        "책에서는 세 개안을 제안하고 있음\n",
        "1. GPU 구입\n",
        "2. Cloud 환경 구독\n",
        "3. Google Colab\n",
        "\n",
        "간단하게는 colab에서 구동할 수 있으며,  \n",
        "cloud는 큰 모델 혹은 오래쓰기엔 너무 요금이 많이 나와 GPU 구입을 권장하는 편."
      ],
      "metadata": {
        "id": "GCg3pUGzz76u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 텐서플로 시작하기\n",
        "\n",
        "tf API에서는 다음의 기능을 수행\n",
        "- 텐서 정의 : 변수(신경망 상태 저장하는 텐서) 포함\n",
        "- 저수준 텐서 연산 : relu, matmul, dot...\n",
        "- 역전파(with GradientTape)\n",
        "\n",
        "keras API에서는 고수준의 딥러닝 개념이 구현될 수 있도록 함\n",
        "- layer : 모델 구성\n",
        "- loss function : 학습 피드백 신호 정의\n",
        "- optimizer : 학습 진행 방향 결정\n",
        "- metrics : 모델 성능 평가\n",
        "- Training loop : 미니 배치 확률적 경사 하강법 수행"
      ],
      "metadata": {
        "id": "mjidjjEh4THj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 상수 텐서와 변수\n",
        "\n",
        "- 넘파이 배열과 텐서플로 텐서의 차이점 : 텐서는 값 할당이 불가능\n",
        "- 변수(tf.Variable) : 값 할당을 위한 특별한 텐서, assign 메소드를 이용하여 수정할 수 있음\n",
        "  > weigth, bias 와 같이 계속적으로 변해야하는 값에 호출\n"
      ],
      "metadata": {
        "id": "aGxG7zqb-Q2m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "071WzbAQzw9A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 텐서 연산 : 텐서플로에서 수학 계산하기\n",
        "\n",
        "텐서 연산은 eager excuttion mode로 수행되어 바로 결과값이 출력\n",
        "\n",
        "> **eager excution**  \n",
        "tf 1.x 에서는 계산 그래프 선언 > session 수행 이라는 과정을 겪어야 텐서들이 계산 되었음\n",
        "그 과정에서 tf.placeholder에 입력값을 주고 feed_dict에 노드 값을 대입하는 불편함이 있었으나 tf 2.0 이후로는 이 과정 없이 넘파이 연산처럼 즉시 결과 값이 출력 됨  \n",
        "출처 : https://m.blog.naver.com/beyondlegend/222140607467"
      ],
      "metadata": {
        "id": "XMGQXSxT-6hq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zJTvo2lHAiX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5.3. GradientTape API 다시 살펴보기\n",
        "\n",
        "- numpy와의 큰 차별점\n",
        "- 미분 가능한 표현이라면 어떤 입력이라도 gradient 계산 가능\n",
        "- 일반적으로 훈련가능한 `변수`의 gradient를 계산\n",
        "- 불필요한 연산을 막기위하여 변수들 만을 추적하기 때문에 상수는 tape.watch()를 이용하여 별도로 신호를 주어야 추적할 수 있음.\n",
        "- second-order gradient도 계산 가능\n"
      ],
      "metadata": {
        "id": "NQHS6OkuAz4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5U3vWqDWB7xm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.4 엔드-투-엔드 예제 : 텐서플로 선형 분류\n",
        "\n",
        "**project 순서**\n",
        "1. 특정한 평균과 공분산 행렬을 가진 랜덤한 분포 좌표들을 가진 2개 클래스를 생성\n",
        "  > - 평균 : 평면에서의 위치 결정  \n",
        "  > - 공분산 행렬 : 포인트 클라우드의 형태 결정\n",
        "2. 변수 W는 랜덤한 값으로, 변수 b는 0으로 초기화 하여 정의\n",
        "  > 좌표 값을 가지는 2D matrix이므로 W는 x, y에 대한 2개의 스칼라값을 가지는 벡터로 생성\n",
        "3. 정방향 패스를 위한 함수 정의\n",
        "4. 손실함수 정의\n",
        "5. training step 정의 (tf.GradientTape 이용)\n",
        "6. 훈련 (Epoch = 40)\n",
        "7. 결과 시각화 (최종 수렴한 함수 포함)\n"
      ],
      "metadata": {
        "id": "2hr02Ah9B8dK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FvVMp2j1E73o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 신경망의 구조 : 핵심 Keras API 이해하기\n",
        "\n",
        "### 3.6.1 층 : 딥러닝의 구성 요소\n",
        "\n",
        "하나 이상의 텐서를 입력 받고 하나 이상의 텐서를 출력하는 데이터 처리 모듈\n",
        "\n",
        "일부 layer (Flatten, Dropout 등) 제외하고는 weight라는 상태를 포함하고 있음\n",
        "\n",
        "weight는 확률적 경사 하강법으로 학습되는 하나 이상의 텐서를 의미\n",
        "\n",
        "층 마다 텐서 포맷과 처리 방식이 다름\n",
        "  > rank-2 tensor :  \n",
        "  Dense layer를 이용하여 처리하는 경우가 많음  \n",
        "      - 밀집 연결 층, 완전 연결 층으로도 부름\n",
        "\n",
        "  > rank-3 tensor :  \n",
        "  LSTM과 같은 recurrent layer, Convolution 1-D layer를 이용하여 처리\n",
        "\n",
        "  > rank-4 tensor:\n",
        "  Conv2D로 처리\n"
      ],
      "metadata": {
        "id": "ZfaS4wFRGrgB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## keras Layer Class\n",
        "\n",
        "**keras API의 중심**\n",
        "\n",
        "Layer는 `상태와 연산(정방향 패스)을 캡슐화 한 객체`로 정의  \n",
        "- 상태 생성을 위한 전용 메서드인 build() 에서 weight 정의  \n",
        "  > 해당 메서드는 \\_\\_call__()을 통하여 자동으로 자동으로 호출\n",
        "  ```python\n",
        "  **기본 layer class**\n",
        "  def __call__(self,inputs):\n",
        "    if not self.built:\n",
        "      self.build(inputs.shape)\n",
        "      self.built = True\n",
        "    return self.call(inputs)\n",
        "  ```\n",
        "- 호출 메서드 call() 에서 연산을 정의함"
      ],
      "metadata": {
        "id": "UqH1WlA9Kx_y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BHwCtkLCKxUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 자동 크기 추론 : 동적으로 층 만들기\n",
        "\n",
        "층 호환 (layer compatibility)\n",
        "- 모든 층은 특정 크기의 입력 텐서만 받고, 특정 크기의 출력 텐서만 반환\n",
        "\n",
        "keras는 동적으로 앞선 층의 크기에 맞춰 layer를 구성함\n",
        "\n",
        "이로 인해 코드가 간단하고 깔끔하게 표현 가능해짐\n",
        "\n",
        "```python\n",
        "model = keras.Sequenctial([\n",
        "  SimpleDense(32,activation = 'relu'),\n",
        "  SimpleDense(64,activation = 'relu'),\n",
        "  SimpleDense(32,activation = 'relu'),\n",
        "  SimpleDense(10,activation = 'softmax'),\n",
        "])\n",
        "```"
      ],
      "metadata": {
        "id": "FMBBXyccKzmU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RYgYlO6tNSuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.2 층에서 모델로\n",
        "\n",
        "딥러닝 모델 == `층으로 구성된 그래프`\n",
        "\n",
        "keras에서는 Model Class로 구현해 두었음\n",
        "\n",
        "- Sequential (한 개의 input을 받는다는 제한 존재)\n",
        "- Functional\n",
        "- Subclass\n",
        "\n",
        "모델의 구조는 `가설 공간(hypothesis space)`을 정의\n",
        "\n",
        ": 가능성 있는 공간에서 입력 데이터를 출력 데이터로 매핑하는 일련의 텐서 연산 -> 최적의 weight를 찾는 것의 목표\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H5H72vlCNom1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6.3 '컴파일' 단계 : 학습 과정 설정\n",
        "\n",
        "모델 구조 정의 후 필요한 단계들을 설정하는 단계\n",
        "- loss function\n",
        "- optimizer\n",
        "- metric\n",
        "\n",
        "keras에서는 간단하게 지정어를 입력하여 손쉽게 정의 할 수 있음  \n",
        "(e.g. optimizer = 'adam')\n",
        "\n",
        "인스턴스 객체로 지정로 부르게 되면 좀 더 상세한 제어 및 custom 가능\n",
        "(e.g. optimizer = keras.optimizers.Adam(learning_rate = 1e-2))\n",
        "\n",
        "### 3.6.4 손실 함수 선택하기\n",
        "\n",
        "`To be continued`"
      ],
      "metadata": {
        "id": "DlqlhyuCOsf-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0tEU8kHROqnL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}